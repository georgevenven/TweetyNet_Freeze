{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "# import torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "num_songs_per_folder = 250\n",
    "precent_data_used = .25\n",
    "window_size = 88\n",
    "num_classes = 100\n",
    "training_dir = '/home/george-vengrovski/Documents/tweetynet_freeze/tweetynet_freeze/data/training'\n",
    "holdout_dir = '/home/george-vengrovski/Documents/tweetynet_freeze/tweetynet_freeze/data/holdout'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Generating Training Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## slice up into windows\n",
    "# dir = '/home/george-vengrovski/Documents/tweetynet_freeze/backup/training'\n",
    "\n",
    "# training_folders = glob.glob(dir + '/*')\n",
    "\n",
    "# print(training_folders)\n",
    "\n",
    "# for folder in training_folders:\n",
    "#     songs = glob.glob(folder + '/*')\n",
    "#     # for sorted \n",
    "#     songs.sort()\n",
    "#     for i, song in enumerate(songs):\n",
    "#         if i > num_songs_per_folder:\n",
    "#             break\n",
    "#         name = song.split('/')[-1]\n",
    "#         print(name)\n",
    "#         song = np.load(song)\n",
    "#         for slice in range(0, song.shape[1] - window_size, window_size):\n",
    "#             window = song[:,slice:slice+window_size]\n",
    "#             print(window.shape)\n",
    "#             np.save(training_dir + '/' + str(slice)+ '_'+ name , window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_tensor_fn(batch):\n",
    "    max_length = 0\n",
    "\n",
    "    # Find the maximum length along the third dimension\n",
    "    for b in batch:\n",
    "        if b[0].shape[2] > max_length:\n",
    "            max_length = b[0].shape[2]\n",
    "\n",
    "    # Pad tensors to the maximum length\n",
    "    padded_batch = []\n",
    "    for b in batch:\n",
    "        tensor, labels = b\n",
    "        padded_tensor = F.pad(tensor, (0, max_length - tensor.shape[2]))\n",
    "        padded_labels = F.pad(labels, (0, max_length - labels.shape[2]))\n",
    "        padded_batch.append((padded_tensor, padded_labels))\n",
    "\n",
    "    # Stack the padded tensors\n",
    "    specs = torch.stack([b[0] for b in padded_batch])\n",
    "    labels = torch.stack([b[1] for b in padded_batch])\n",
    "\n",
    "    return specs, labels\n",
    "\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, train_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.specs = []\n",
    "        self.labels = []\n",
    "    \n",
    "        for file in sorted(os.listdir(train_dir)):\n",
    "            file_type = file.split('_')[-1]\n",
    "            file_type = file_type.split('.')[0]\n",
    "            if file_type == 'spec':\n",
    "                spec = np.load(training_dir + '/' + file)\n",
    "                file_name = file.split('_')[0:-1]\n",
    "                file_name = '_'.join(file_name)\n",
    "                labels = np.load(training_dir + '/' + file_name + '_labels.npy')\n",
    "                # add dimension of 1 to spec \n",
    "                spec = np.expand_dims(spec, axis=0)\n",
    "                self.specs.append(spec)\n",
    "                self.labels.append(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        specs = self.specs[index]\n",
    "        labels = self.labels[index]\n",
    "\n",
    "        labels = self.one_hot(labels)\n",
    "        # swap dim 1 and dim 2\n",
    "        labels = np.swapaxes(labels, 1, 2)\n",
    "\n",
    "        if self.transform:\n",
    "            specs = self.transform(specs)\n",
    "            \n",
    "        return (specs, labels)\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.specs)\n",
    "    \n",
    "    def one_hot(self, arr):\n",
    "        return F.one_hot(torch.from_numpy(arr).long(), num_classes=num_classes).float()\n",
    "\n",
    "# Transform function to convert numpy arrays to tensors\n",
    "def to_tensor(arr):\n",
    "    return torch.from_numpy(arr).float()\n",
    "\n",
    "# Create the transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    to_tensor  # Convert numpy arrays to tensors\n",
    "])\n",
    "\n",
    "# Create the dataset without the transform\n",
    "train_dataset = NumpyDataset(train_dir=training_dir, transform=transform)\n",
    "\n",
    "# Use the dataset in the data loader with the collate_fn argument\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_tensor_fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dTF(nn.Conv2d):\n",
    "\n",
    "    PADDING_METHODS = ('VALID', 'SAME')\n",
    "\n",
    "    \"\"\"Conv2d with padding behavior from Tensorflow\n",
    "    adapted from\n",
    "    https://github.com/mlperf/inference/blob/16a5661eea8f0545e04c86029362e22113c2ec09/others/edge/object_detection/ssd_mobilenet/pytorch/utils.py#L40\n",
    "    as referenced in this issue:\n",
    "    https://github.com/pytorch/pytorch/issues/3867#issuecomment-507025011\n",
    "    used to maintain behavior of original implementation of TweetyNet that used Tensorflow 1.0 low-level API\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # remove 'padding' from ``kwargs`` to avoid bug in ``torch`` => 1.7.2\n",
    "        # see https://github.com/yardencsGitHub/tweetynet/issues/166\n",
    "        kwargs_super = {k: v for k, v in kwargs.items() if k != 'padding'}\n",
    "        super(Conv2dTF, self).__init__(*args, **kwargs_super)\n",
    "        padding = kwargs.get(\"padding\", \"SAME\")\n",
    "        if not isinstance(padding, str):\n",
    "            raise TypeError(f\"value for 'padding' argument should be a string, one of: {self.PADDING_METHODS}\")\n",
    "        padding = padding.upper()\n",
    "        if padding not in self.PADDING_METHODS:\n",
    "            raise ValueError(\n",
    "                f\"value for 'padding' argument must be one of '{self.PADDING_METHODS}' but was: {padding}\"\n",
    "            )\n",
    "        self.padding = padding\n",
    "\n",
    "    def _compute_padding(self, input, dim):\n",
    "        input_size = input.size(dim + 2)\n",
    "        filter_size = self.weight.size(dim + 2)\n",
    "        effective_filter_size = (filter_size - 1) * self.dilation[dim] + 1\n",
    "        out_size = (input_size + self.stride[dim] - 1) // self.stride[dim]\n",
    "        total_padding = max(\n",
    "            0, (out_size - 1) * self.stride[dim] + effective_filter_size - input_size\n",
    "        )\n",
    "        additional_padding = int(total_padding % 2 != 0)\n",
    "\n",
    "        return additional_padding, total_padding\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.padding == \"VALID\":\n",
    "            return F.conv2d(\n",
    "                input,\n",
    "                self.weight,\n",
    "                self.bias,\n",
    "                self.stride,\n",
    "                padding=0,\n",
    "                dilation=self.dilation,\n",
    "                groups=self.groups,\n",
    "            )\n",
    "        elif self.padding == \"SAME\":\n",
    "            rows_odd, padding_rows = self._compute_padding(input, dim=0)\n",
    "            cols_odd, padding_cols = self._compute_padding(input, dim=1)\n",
    "            if rows_odd or cols_odd:\n",
    "                input = F.pad(input, [0, cols_odd, 0, rows_odd])\n",
    "\n",
    "            return F.conv2d(\n",
    "                input,\n",
    "                self.weight,\n",
    "                self.bias,\n",
    "                self.stride,\n",
    "                padding=(padding_rows // 2, padding_cols // 2),\n",
    "                dilation=self.dilation,\n",
    "                groups=self.groups,\n",
    "            )\n",
    "\n",
    "class TweetyNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 input_shape=(1, 513, 88),\n",
    "                 padding='SAME',\n",
    "                 conv1_filters=32,\n",
    "                 conv1_kernel_size=(5, 5),\n",
    "                 conv2_filters=64,\n",
    "                 conv2_kernel_size=(5, 5),\n",
    "                 pool1_size=(8, 1),\n",
    "                 pool1_stride=(8, 1),\n",
    "                 pool2_size=(8, 1),\n",
    "                 pool2_stride=(8, 1),\n",
    "                 hidden_size=None,\n",
    "                 rnn_dropout=0.,\n",
    "                 num_layers=1,\n",
    "                 bidirectional=True,\n",
    "                 ):\n",
    "        \"\"\"initialize TweetyNet model\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            number of classes to predict, e.g., number of syllable classes in an individual bird's song\n",
    "        input_shape : tuple\n",
    "            with 3 elements corresponding to dimensions of spectrogram windows: (channels, frequency bins, time bins).\n",
    "            i.e. we assume input is a spectrogram and treat it like an image, typically with one channel,\n",
    "            the rows are frequency bins, and the columns are time bins. Default is (1, 513, 88).\n",
    "        padding : str\n",
    "            type of padding to use, one of {\"VALID\", \"SAME\"}. Default is \"SAME\".\n",
    "        conv1_filters : int\n",
    "            Number of filters in first convolutional layer. Default is 32.\n",
    "        conv1_kernel_size : tuple\n",
    "            Size of kernels, i.e. filters, in first convolutional layer. Default is (5, 5).\n",
    "        conv2_filters : int\n",
    "            Number of filters in second convolutional layer. Default is 64.\n",
    "        conv2_kernel_size : tuple\n",
    "            Size of kernels, i.e. filters, in second convolutional layer. Default is (5, 5).\n",
    "        pool1_size : two element tuple of ints    specs = self.transform(specs)\n",
    "\n",
    "            Size of sliding window for first max pooling layer. Default is (1, 8)\n",
    "        pool1_stride : two element tuple of ints\n",
    "            Step size for sliding window of first max pooling layer. Default is (1, 8)\n",
    "        pool2_size : two element tuple of ints\n",
    "            Size of sliding window for second max pooling layer. Default is (1, 8),\n",
    "        pool2_stride : two element tuple of ints\n",
    "            Step size for sliding window of second max pooling layer. Default is (1, 8)\n",
    "        hidden_size : int\n",
    "            number of features in the hidden state ``h``. Default is None,\n",
    "            in which case ``hidden_size`` is set to the dimensionality of the\n",
    "            output of the convolutional neural network. This default maintains\n",
    "            the original behavior of the network.\n",
    "        rnn_dropout : float\n",
    "            If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer,\n",
    "            with dropout probability equal to dropout. Default: 0\n",
    "        num_layers : int\n",
    "            Number of recurrent layers. Default is 1.\n",
    "        bidirectional : bool\n",
    "            If True, make LSTM bidirectional. Default is True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            Conv2dTF(in_channels=self.input_shape[0],\n",
    "                     out_channels=conv1_filters,\n",
    "                     kernel_size=conv1_kernel_size,\n",
    "                     padding=padding\n",
    "                     ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=pool1_size,\n",
    "                         stride=pool1_stride),\n",
    "            Conv2dTF(in_channels=conv1_filters,\n",
    "                     out_channels=conv2_filters,\n",
    "                     kernel_size=conv2_kernel_size,\n",
    "                     padding=padding,\n",
    "                     ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=pool2_size,\n",
    "                         stride=pool2_stride),\n",
    "        )\n",
    "\n",
    "        # determine number of features in output after stacking channels\n",
    "        # we use the same number of features for hidden states\n",
    "        # note self.num_hidden is also used to reshape output of cnn in self.forward method\n",
    "        batch_shape = tuple((1,) + input_shape)\n",
    "        tmp_tensor = torch.rand(batch_shape)\n",
    "        tmp_out = self.cnn(tmp_tensor)\n",
    "        channels_out, freqbins_out = tmp_out.shape[1], tmp_out.shape[2]\n",
    "        self.rnn_input_size = channels_out * freqbins_out\n",
    "\n",
    "        if hidden_size is None:\n",
    "            self.hidden_size = self.rnn_input_size\n",
    "        else:\n",
    "            self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=self.rnn_input_size,\n",
    "                           hidden_size=self.hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           dropout=rnn_dropout,\n",
    "                           bidirectional=bidirectional)\n",
    "\n",
    "        # for self.fc, in_features = hidden_size * 2 because LSTM is bidirectional\n",
    "        # so we get hidden forward + hidden backward as output\n",
    "        self.fc = nn.Linear(in_features=self.hidden_size * 2, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        # stack channels, to give tensor shape (batch, rnn_input_size, num time bins)\n",
    "        features = features.view(features.shape[0], self.rnn_input_size, -1)\n",
    "        # switch dimensions for feeding to rnn, to (num time bins, batch size, input size)\n",
    "        features = features.permute(2, 0, 1)\n",
    "        rnn_output, _ = self.rnn(features)\n",
    "        # permute back to (batch, time bins, hidden size) to project features down onto number of classes\n",
    "        rnn_output = rnn_output.permute(1, 0, 2)\n",
    "        logits = self.fc(rnn_output)\n",
    "        # permute yet again so that dimension order is (batch, classes, time steps)\n",
    "        # because this is order that loss function expects\n",
    "        return logits.permute(0, 2, 1)\n",
    "    \n",
    "    def loss_function(self, y_pred, y_true):\n",
    "        \"\"\"loss function for TweetyNet\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : torch.Tensor\n",
    "            output of TweetyNet model, shape (batch, time bins, classes)\n",
    "        y_true : torch.Tensor\n",
    "            one-hot encoded labels, shape (batch, time bins, classes)\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            mean cross entropy loss\n",
    "        \"\"\"\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        return loss(y_pred, y_true)\n",
    "    \n",
    "    def acc(self, y_pred, y):\n",
    "        return y_pred.eq(y.view_as(y_pred)).sum().item() / np.prod(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4357092\n",
      "Training ...\n",
      "Epoch 1/100\n",
      "Epoch: 0\n",
      "torch.Size([16, 100, 88])\n",
      "torch.Size([16, 100, 88])\n",
      "train loss: 0.4820121472982637\n",
      "accuracy: 0.0\n",
      "Epoch 2/100\n",
      "Epoch: 1\n",
      "torch.Size([16, 100, 88])\n",
      "torch.Size([16, 100, 88])\n",
      "train loss: 0.13711462138702338\n",
      "accuracy: 0.0\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m spec \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m prediction \u001b[39m=\u001b[39m model(spec)\n\u001b[1;32m     26\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 178\u001b[0m, in \u001b[0;36mTweetyNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m# switch dimensions for feeding to rnn, to (num time bins, batch size, input size)\u001b[39;00m\n\u001b[1;32m    177\u001b[0m features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m rnn_output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(features)\n\u001b[1;32m    179\u001b[0m \u001b[39m# permute back to (batch, time bins, hidden size) to project features down onto number of classes\u001b[39;00m\n\u001b[1;32m    180\u001b[0m rnn_output \u001b[39m=\u001b[39m rnn_output\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/canary-vae/lib/python3.11/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TweetyNet(num_classes=100, input_shape=(1, 513, 88))\n",
    "use_gpu = True\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "train_loss_for_plot = []\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch %d/%d' % (epoch+1, num_epochs))\n",
    "\n",
    "    train_loss_avg = []\n",
    "\n",
    "    for spec, labels in train_loader:\n",
    "        # list to tensor\n",
    "        spec = spec.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        prediction = model(spec)\n",
    "\n",
    "        labels = labels.squeeze(1)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        loss = model.loss_function(prediction, labels)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # one step of the optmizer (using the gradients from backpropagation)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg.append(loss.item())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), '/home/george-vengrovski/Documents/tweetynet_freeze/tweetynet_freeze/training_checkpoints/checkpoint{}'.format(epoch))\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(prediction.shape)\n",
    "    print(labels.shape)\n",
    "    print(f\"train loss: {np.mean(train_loss_avg)}\")\n",
    "    print(f\"accuracy: {model.acc(prediction, labels)}\")\n",
    "\n",
    "    train_loss_for_plot.append(np.mean(train_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2b300a8250>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_for_plot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# get random test sample\n",
    "spec, labels = next(iter(train_loader))\n",
    "spec = spec.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# forward pass\n",
    "prediction = model(spec)\n",
    "\n",
    "# print('Prediction shape:', prediction[0].T.shape)\n",
    "# print(prediction)\n",
    "print(labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
